{
 "metadata": {
  "kernelspec": {
   "display_name": "summit-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  },
  "lastEditStatus": {
   "notebookId": "qboapi74y5wsacsegmj4",
   "authorId": "7698903671161",
   "authorName": "HOL_USER_999",
   "authorEmail": "",
   "sessionId": "91ec630a-f4a9-42da-b7f9-ea0a8e8670a8",
   "lastEditTime": 1749072857335
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60f6c3d-8985-4733-9197-4ffc21b4dbce",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "***This is a Snowflake Notebook. Please import this notebook in Snowflake Notebook UI to continue running the BRONZE ingestion pipeline***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76883fdc-3bc9-4fce-adee-d6b8b1a56175",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook guides you through the setup and execution of a BRONZE ingestion pipeline using Snowflake and Iceberg tables. It covers the following steps:\n",
    "\n",
    "- Importing required Python packages and establishing a Snowflake session.\n",
    "- Setting up user-specific variables, roles, databases, and schemas.\n",
    "- Creating file formats for ingesting different types of files such as CSV and Parquet for batch data ingestion.\n",
    "- Different ingestion patterns supported by Snowflake for loading data into Snowflake Iceberg tables\n",
    "- Creating and loading raw data tables (locations, schedules, cancel codes, and movements) using Iceberg.\n",
    "- Providing SQL and Python code examples for data ingestion and exploration.\n",
    "\n",
    "Follow the instructions and code cells to complete the ingestion pipeline and prepare your data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d872c-dc1c-4eef-add1-db73225fef8e",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "# Set User Number\n",
    "\n",
    "Update the cell below with your unique user number for this lab. This ensures that all resources you create are isolated and do not conflict with those of other users.\n",
    "\n",
    "**usernum = '999'** or **usernum = '001'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d553fc-487c-49c6-82b8-172d479f4c66",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "usernum = str('<INSERT USER NUMBER HERE>')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "SET USERNAME = 'HOL_USER_' || '{{usernum}}';\n",
    "SELECT $USERNAME;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022a723-f42d-4770-9210-b73bbe2668f9",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "SET HOLROLE = $USERNAME || '_FULL_ROLE';\n",
    "SET DB_NAME = $USERNAME || '_DB';\n",
    "SET SCHEMANAME = 'BRONZE';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba547683-4d5c-49bf-9409-f288129d91cd",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "USE ROLE IDENTIFIER($HOLROLE);\n",
    "USE DATABASE IDENTIFIER($DB_NAME);\n",
    "USE SCHEMA IDENTIFIER($SCHEMANAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345d412-e2a3-48fb-9f13-6ccba8c76896",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5097c7df-9f86-4724-af9f-02b7318cc053",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "# Create File Formats for Data Ingestion\n",
    "\n",
    "The following cell creates a file format named `CSV_FORMAT` in Snowflake. This file format is configured to handle CSV files with specific parsing options, such as parsing headers, using a comma as the field delimiter, trimming spaces, and treating empty fields as null values. This format will be used for ingesting CSV data into Snowflake tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FILE FORMAT CSV_FORMAT\n",
    "TYPE = CSV\n",
    "PARSE_HEADER = TRUE\n",
    "FIELD_DELIMITER = ','\n",
    "TRIM_SPACE = TRUE\n",
    "NULL_IF = ( 'null')\n",
    "EMPTY_FIELD_AS_NULL = TRUE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46626274-0177-4fe4-b821-1296464dc9d7",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "# Create File Format for Parquet Data Ingestion\n",
    "\n",
    "The following cell creates a file format named `parquet_format` in Snowflake. This format will be used for ingesting Parquet data into Snowflake Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0cd9d-33f8-4df9-990b-81348721bb02",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FILE FORMAT parquet_format\n",
    "  TYPE = PARQUET\n",
    "  USE_VECTORIZED_SCANNER = TRUE\n",
    "  USE_LOGICAL_TYPE = TRUE\n",
    "  BINARY_AS_TEXT = TRUE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d63114-9867-4641-8460-daf29e37d569",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    },
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Creating a Snowflake Stage for Raw File Ingestion\n",
    "\n",
    "A Snowflake stage is created to facilitate the ingestion of raw files that are pushed to the landing zone. The stage uses a storage integration, which provides a secure and managed access channel to an external S3 bucket. This ensures that data can be ingested efficiently and securely without exposing sensitive credentials.\n",
    "\n",
    "**Note:** The storage integration (`s3_iceberg_hol_int`) has already been pre-created as part of the environment setup, so you do not need to configure it manually.\n",
    "\n",
    "For more details on storage integrations in Snowflake, refer to the official documentation: [Snowflake Storage Integration](https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91a665-7631-43a4-971d-f4a3a10332d1",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "--create external stage\n",
    "CREATE OR REPLACE STAGE ICEBRG_HOL_STG\n",
    "  STORAGE_INTEGRATION = s3_iceberg_hol_int\n",
    "  URL = 's3://sf-iceberg-hol-landing/';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e50b9-249c-4f5f-a264-0ad6089f3354",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "# Creating the LOCATIONS_RAW Table\n",
    "\n",
    "The following step creates an Iceberg table named `LOCATIONS_RAW` in Snowflake. This table is designed to store raw location data, which will be ingested from CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2aa13-d8e2-4a49-9520-6bcf6ba5baf0",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# create locations bronze table. \n",
    "session.sql(\"\"\"\n",
    "create or replace iceberg table locations_raw (\n",
    "location_id varchar,\n",
    "name varchar,\n",
    "description varchar,\n",
    "tiploc varchar,\n",
    "crs varchar,\n",
    "nlc varchar,\n",
    "stanox varchar,\n",
    "notes varchar,\n",
    "longitude varchar,\n",
    "latitude varchar,\n",
    "isOffNetwork varchar,\n",
    "timingPointType varchar\n",
    ")\n",
    "EXTERNAL_VOLUME = 'iceberg_hol_bronze_vol'\n",
    "CATALOG = 'SNOWFLAKE'\n",
    "BASE_LOCATION = 'hol_user_\"\"\" + usernum + \"\"\"/locations_raw/'\n",
    "catalog_sync = iceberg_hol_oc_int;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8816fda-01a8-426a-a40c-9a3fbb333edc",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "--Data is loaded from csv files in landing stage\n",
    "\n",
    "COPY INTO locations_raw\n",
    "from @ICEBRG_HOL_STG/openraildata-talk-carl-partridge-ukrail_locations.csv\n",
    "FILE_FORMAT = CSV_FORMAT\n",
    "MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd4899-1b38-40f9-b424-648ab36a92b5",
   "metadata": {
    "name": "cell24"
   },
   "source": [
    "# Creating the SCHEDULE_RAW Table\n",
    "\n",
    "The following step creates an Iceberg table named `SCHEDULE_RAW` in Snowflake. This table is designed to store raw schedule data ingested from Parquet files. \n",
    "\n",
    "**Note:** The `SCHEDULE_RAW` table contains nested columns (such as arrays and objects) to accommodate complex schedule data structures. These nested columns will be flattened and transformed in subsequent pipeline steps to facilitate easier querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275ff09-bde6-474a-9730-300fefd5a7e4",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "--create schedule table\n",
    "\n",
    "CREATE OR REPLACE ICEBERG TABLE SCHEDULE_RAW (\n",
    "\tCIF_BANK_HOLIDAY_RUNNING STRING,\n",
    "\tCIF_STP_INDICATOR STRING,\n",
    "\tCIF_TRAIN_UID STRING,\n",
    "\tAPPLICABLE_TIMETABLE STRING,\n",
    "\tATOC_CODE STRING,\n",
    "\tNEW_SCHEDULE_SEGMENT OBJECT(traction_class STRING, uic_code STRING),\n",
    "\tSCHEDULE_DAYS_RUNS STRING,\n",
    "\tSCHEDULE_END_DATE DATE,\n",
    "\tSCHEDULE_SEGMENT OBJECT(signalling_id STRING, CIF_train_category STRING, CIF_headcode STRING, CIF_course_indicator STRING, CIF_train_service_code STRING, CIF_business_sector STRING, CIF_power_type STRING, CIF_timing_load STRING, CIF_speed STRING, CIF_operating_characteristics STRING, CIF_train_class STRING, CIF_sleepers STRING, CIF_reservations STRING, CIF_connection_indicator STRING, CIF_catering_code STRING, CIF_service_branding STRING, schedule_location ARRAY(OBJECT(arrival STRING, departure STRING, engineering_allowance STRING, line STRING, pass STRING, path STRING, location_type STRING, pathing_allowance STRING, performance_allowance STRING, platform STRING, public_arrival STRING, public_departure STRING, record_identity STRING, tiploc_code STRING, tiploc_instance STRING))),\n",
    "\tSCHEDULE_START_DATE DATE,\n",
    "\tTRAIN_STATUS STRING,\n",
    "\tTRANSACTION_TYPE STRING\n",
    ")\n",
    " EXTERNAL_VOLUME = 'iceberg_hol_bronze_vol'\n",
    " CATALOG = 'SNOWFLAKE'\n",
    " BASE_LOCATION = 'hol_user_{{usernum}}/SCHEDULE_RAW/'\n",
    " catalog_sync = iceberg_hol_oc_int;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3a322-3f23-465e-92f9-e01c69f6f0c9",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "## Understanding the `load_mode` Parameter in the `COPY INTO` Command\n",
    "\n",
    "The `load_mode` parameter in the `COPY INTO` command controls how files are ingested into Snowflake tables. When working with Parquet files and Iceberg tables, setting `load_mode = add_files_copy` enables a lightweight and cost-effective ingestion process.\n",
    "\n",
    "**Key Benefits of `ADD_FILES_COPY`:**\n",
    "- **Lightweight Ingestion:** Instead of physically copying data, Snowflake registers the Parquet files' metadata, making the ingestion process much faster.\n",
    "- **Cost Effective:** Since data is not duplicated, storage costs are minimized and compute usage is reduced.\n",
    "- **Efficient for Large Datasets:** Ideal for scenarios where large volumes of Parquet files need to be ingested quickly into Iceberg tables.\n",
    "\n",
    "This approach is especially useful for data lakes and big data workflows, where minimizing data movement and optimizing resource usage are critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702b6d2-23e2-4e4f-a378-b5084c863f81",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "\n",
    "COPY INTO SCHEDULE_RAW\n",
    "from @ICEBRG_HOL_STG/train_schedule/\n",
    "FILE_FORMAT = parquet_format\n",
    "load_mode = add_files_copy\n",
    "MATCH_BY_COLUMN_NAME = CASE_SENSITIVE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c642ea1-e65d-45f4-934c-e8b7fc8a6572",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "# Reading Delta Tables In-Place with Snowflake\n",
    "\n",
    "Snowflake supports reading Delta Lake tables directly in place, without requiring any data copy or movement. This capability allows you to query and analyze data stored in Delta format on external storage, leveraging Snowflake's compute and security features.\n",
    "\n",
    "**Key Points:**\n",
    "- **No Data Copy Required:** Snowflake can read Delta tables in place, eliminating the need for time-consuming and costly data duplication.\n",
    "- **External Volume & Catalog Integrations:** The required external volume and catalog integrations have already been created as part of the environment setup.\n",
    "- **Iceberg Metadata Generation:** When the external volume is created with the `ALLOW_WRITE` property set to `TRUE`, Snowflake can automatically generate Iceberg metadata for Delta tables, enabling seamless interoperability\n",
    "\n",
    "For more information, refer to the official documentation:\n",
    "- [External Volumes in Snowflake](https://docs.snowflake.com/en/user-guide/data-external-volumes)\n",
    "- [Catalog Integrations in Snowflake](https://docs.snowflake.com/en/user-guide/catalog-integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70e512-f161-4946-a5c0-08872acadff3",
   "metadata": {
    "language": "sql",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE OR REPLACE ICEBERG TABLE CANCEL_CODE_REFERENCES_RAW\n",
    "  CATALOG = ICEBERG_HOL_DELTA_INT\n",
    "  EXTERNAL_VOLUME = iceberg_hol_bronze_vol\n",
    "  BASE_LOCATION = 'cancel_code_references_raw/cancel_codes/'\n",
    "  AUTO_REFRESH = TRUE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a878236-3e3c-43c0-8f76-416bc310af4a",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "select * from CANCEL_CODE_REFERENCES_RAW limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35b7ec-718e-46f9-a50c-a5de00d92b66",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "# Creating MOVEMENTS_RAW_00X Tables\n",
    "\n",
    "The following cell creates three Iceberg tables: `MOVEMENTS_RAW_0001`, `MOVEMENTS_RAW_0002`, and `MOVEMENTS_RAW_0003`. These tables are designed to store different types of raw train event data:\n",
    "\n",
    "- **MOVEMENTS_RAW_0001:** Holds train activations event data.\n",
    "- **MOVEMENTS_RAW_0002:** Holds train cancellations event data.\n",
    "- **MOVEMENTS_RAW_0003:** Holds train movements event data.\n",
    "\n",
    "While the table structures are defined in the next cell, the actual data will be loaded into these tables using the Snowflake Openflow service, which enables efficient and scalable ingestion of streaming event data.\n",
    "\n",
    "The source data for these tables originates from a Kafka environment hosted within Snowflake Container Services. A producer program pulls events from the Network Rail message queue and pushes the messages to the Kafka broker, from where they are ingested into the respective tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6214b2-f2a0-42b4-8b50-d3040d2c696b",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create or replace ICEBERG TABLE MOVEMENTS_RAW_0001 (\n",
    "\t\"VALUE\" VARCHAR\n",
    ")\n",
    "EXTERNAL_VOLUME = 'iceberg_hol_bronze_vol'\n",
    "CATALOG = 'SNOWFLAKE'\n",
    "BASE_LOCATION = 'hol_user_{{usernum}}/movements_raw_001/'\n",
    "catalog_sync = iceberg_hol_oc_int;\n",
    "\n",
    "create or replace ICEBERG TABLE MOVEMENTS_RAW_0002 (\n",
    "    \"VALUE\" VARCHAR\n",
    ")\n",
    "EXTERNAL_VOLUME = 'iceberg_hol_bronze_vol'\n",
    "CATALOG = 'SNOWFLAKE'\n",
    "BASE_LOCATION = 'hol_user_{{usernum}}/movements_raw_002/'\n",
    "catalog_sync = iceberg_hol_oc_int;\n",
    "\n",
    "create or replace ICEBERG TABLE MOVEMENTS_RAW_0003 (\n",
    "   \"VALUE\" VARCHAR\n",
    ")\n",
    "EXTERNAL_VOLUME = 'iceberg_hol_bronze_vol'\n",
    "CATALOG = 'SNOWFLAKE'\n",
    "BASE_LOCATION = 'hol_user_{{usernum}}/movements_raw_003/'\n",
    "catalog_sync = iceberg_hol_oc_int;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545924f-4126-462d-ba40-17409ef41ea2",
   "metadata": {
    "name": "cell28"
   },
   "source": [
    "# Accessing the Openflow Service\n",
    "\n",
    "To continue with the lab, please open a new browser tab and navigate to the Openflow service URL provided in your lab login instructions. \n",
    "\n",
    "**Instructions:**\n",
    "- Use the same credentials you used to log in to Snowflake.\n",
    "- The Openflow service URL is included in your lab materials or login instructions.\n",
    "- Follow the guidelines provided in the README file under the `bronze/openflow` directory to run the Openflow pipeline and ingest data.\n"
   ]
  }
 ]
}