{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0643fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark==3.5.4 findspark plotly altair folium pydeck \"nbformat>=4.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num                            = 999999\n",
    "CLIENT_ID                           = \"\"\n",
    "CLIENT_SECRET                       = \"\"\n",
    "PRINCIPAL_ROLE                      = \"\"\n",
    "POLARIS_CATALOG_ACCOUNT_IDENTIFIER  = \"\"\n",
    "POLARIS_WAREHOUSE                   = \"\"\n",
    "REGION                              = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9136186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "PACKAGES = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.9.1\"\n",
    "spark = SparkSession.builder.appName('iceberg_lab')\\\n",
    ".config(\"spark.driver.memory\", \"3g\") \\\n",
    ".config('spark.jars.packages', PACKAGES) \\\n",
    ".config(\"spark.driver.host\", \"localhost\") \\\n",
    ".config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    ".config('spark.sql.iceberg.vectorization.enabled', \"false\") \\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".config('spark.sql.defaultCatalog', 'polaris') \\\n",
    ".config('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.polaris.type', 'rest') \\\n",
    ".config('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation','vended-credentials') \\\n",
    ".config('spark.sql.catalog.polaris.uri',f'https://{POLARIS_CATALOG_ACCOUNT_IDENTIFIER}.snowflakecomputing.com/polaris/api/catalog') \\\n",
    ".config('spark.sql.catalog.polaris.credential',f'{CLIENT_ID}:{CLIENT_SECRET}') \\\n",
    ".config('spark.sql.catalog.polaris.warehouse',POLARIS_WAREHOUSE) \\\n",
    ".config('spark.sql.catalog.polaris.scope',f'PRINCIPAL_ROLE:{PRINCIPAL_ROLE}')\\\n",
    ".config('spark.sql.catalog.polaris.client.region',REGION)\\\n",
    ".config('spark.sql.catalog.snowflake_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO') \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sql(\"show namespaces\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as spark_max, lit\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "MS_IN_MIN = 60000\n",
    "\n",
    "# Load TRAIN_MOVEMENTS DataFrame (replace with your actual loading logic)\n",
    "train_movements = spark.table(f\"HOL_USER_{user_num}_DB.GOLD.TRAIN_MOVEMENTS\")  \n",
    "\n",
    "# 1️⃣ Calculate max_ts\n",
    "max_ts_row = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .agg(spark_max(col(\"ACTUAL_TIMESTAMP\")).alias(\"max_ts_epoch\")) \\\n",
    "    .collect()[0]\n",
    "\n",
    "max_ts_epoch = max_ts_row[\"max_ts_epoch\"]\n",
    "\n",
    "# 2️⃣ Build time spine (48 half-hour buckets)\n",
    "# We'll create a DataFrame with sequence 0..47\n",
    "seq_df = spark.range(0, 48).withColumnRenamed(\"id\", \"seq\")\n",
    "\n",
    "# Create time_bucket column\n",
    "time_spine = seq_df.withColumn(\n",
    "    \"time_bucket\",lit(max_ts_epoch) - (col(\"seq\") * lit(30 * MS_IN_MIN))\n",
    ")\n",
    "\n",
    "# 3️⃣ Prepare TRAIN_MOVEMENTS with timestamp conversion\n",
    "\n",
    "\n",
    "# 4️⃣ Filter TRAIN_MOVEMENTS to only rows in last 24h window\n",
    "filtered_train_movements = train_movements.filter(\n",
    "    (col(\"ACTUAL_TIMESTAMP\") >=  lit(max_ts_epoch) - lit(24 * 60 * MS_IN_MIN))\n",
    "    & (col(\"ACTUAL_TIMESTAMP\").isNotNull())\n",
    ")\n",
    "\n",
    "# 5️⃣ Join time_spine and train_movements\n",
    "joined_df = time_spine.alias(\"t\").join(\n",
    "    filtered_train_movements.alias(\"m\"),\n",
    "    (col(\"m.ACTUAL_TIMESTAMP\") >= col(\"t.time_bucket\")) &\n",
    "    (col(\"m.ACTUAL_TIMESTAMP\") < col(\"t.time_bucket\") + lit(30 * MS_IN_MIN)),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 6️⃣ Group and aggregate\n",
    "result_df = joined_df.groupBy(\n",
    "    col(\"t.time_bucket\").alias(\"time_bucket\"),\n",
    "    col(\"m.VARIATION_STATUS\").alias(\"status\")\n",
    ").count().withColumnRenamed(\"count\", \"arrival_count\")\n",
    "\n",
    "# 7️⃣ Order by time_bucket, status\n",
    "final_df = result_df.orderBy(\"time_bucket\", \"status\")\n",
    "\n",
    "# Show result\n",
    "final_df.select(to_timestamp(col(\"time_bucket\")/1000).alias(\"time_bucket\"),\"status\",\"arrival_count\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, expr, lit, to_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "\n",
    "# TrainMovementsWithLocations\n",
    "\n",
    "train_movements = spark.table(f\"HOL_USER_{user_num}_DB.GOLD.TRAIN_MOVEMENTS\").alias(\"tm\")\n",
    "locations_raw = spark.table(f\"HOL_USER_{user_num}_DB.BRONZE.LOCATIONS_RAW\").alias(\"loc\")\n",
    "\n",
    "# 1️⃣ Calculate max_ts\n",
    "max_ts_row = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .agg(spark_max(col(\"ACTUAL_TIMESTAMP\")).alias(\"max_ts_epoch\")) \\\n",
    "    .collect()[0]\n",
    "\n",
    "max_ts_epoch = max_ts_row[\"max_ts_epoch\"]\n",
    "\n",
    "\n",
    "# 2️⃣ Build time spine (48 half-hour buckets)\n",
    "seq_df = spark.range(0, 48).withColumnRenamed(\"id\", \"seq\")\n",
    "\n",
    "time_spine = seq_df.withColumn(\n",
    "    \"time_bucket\",\n",
    "    lit(max_ts_epoch) - (col(\"seq\") * lit(30 * MS_IN_MIN))\n",
    ")\n",
    "\n",
    "# 3️⃣ Prepare movements_with_name (with timestamp and location name)\n",
    "movements_with_name = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .join(locations_raw, col(\"tm.LOC_STANOX\") == col(\"loc.STANOX\"), how=\"left\") \\\n",
    "    .select(\n",
    "        col(\"tm.ACTUAL_TIMESTAMP\").alias(\"actual_ts\"),\n",
    "        col(\"tm.VARIATION_STATUS\"),\n",
    "        col(\"loc.NAME\").alias(\"location_name\")\n",
    "    )\n",
    "\n",
    "# 4️⃣ Join time spine and movements_with_name\n",
    "joined_df = time_spine.alias(\"t\").join(\n",
    "    movements_with_name.alias(\"m\"),\n",
    "    (col(\"m.actual_ts\") >= col(\"t.time_bucket\")) &\n",
    "    (col(\"m.actual_ts\") < col(\"t.time_bucket\") + lit(30 * MS_IN_MIN)) &\n",
    "    (col(\"m.actual_ts\") >= lit(max_ts_epoch) - lit(24 * 60 * MS_IN_MIN)),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5️⃣ Group and aggregate\n",
    "result_df = joined_df.groupBy(\n",
    "    col(\"t.time_bucket\").alias(\"time_bucket\"),\n",
    "    col(\"m.location_name\"),\n",
    "    col(\"m.VARIATION_STATUS\").alias(\"status\")\n",
    ").count().withColumnRenamed(\"count\", \"arrival_count\")\n",
    "\n",
    "# 6️⃣ Order by time_bucket, location_name, status\n",
    "final_df = result_df.orderBy(\"time_bucket\", \"location_name\", \"status\")\n",
    "\n",
    "# Show result\n",
    "final_df.select(to_timestamp(col(\"time_bucket\")/1000).alias(\"time_bucket\"),\"location_name\",\"status\",\"arrival_count\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as spark_max, to_timestamp, floor\n",
    "\n",
    "# TopStationsDelays\n",
    "train_movements = spark.table(f\"HOL_USER_{user_num}_DB.GOLD.TRAIN_MOVEMENTS\").alias(\"tm\")\n",
    "locations_raw = spark.table(f\"HOL_USER_{user_num}_DB.BRONZE.LOCATIONS_RAW\").alias(\"loc\")\n",
    "\n",
    "# 1️⃣ Calculate max_ts\n",
    "max_ts_row = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .agg(spark_max(col(\"ACTUAL_TIMESTAMP\")).alias(\"max_ts_epoch\")) \\\n",
    "    .collect()[0]\n",
    "\n",
    "max_ts_epoch = max_ts_row[\"max_ts_epoch\"]\n",
    "\n",
    "\n",
    "# 2️⃣ converted_data CTE\n",
    "converted_data = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .withColumn(\"actual_ts\", col(\"ACTUAL_TIMESTAMP\")) \\\n",
    "    .filter(col(\"actual_ts\") >= lit(max_ts_epoch) - lit(24 * 60 * MS_IN_MIN)) \\\n",
    "    .select(\n",
    "        col(\"LOC_STANOX\"),\n",
    "        col(\"actual_ts\"),\n",
    "        col(\"TIMETABLE_VARIATION\"),\n",
    "        col(\"LATE_IND\")\n",
    "    )\n",
    "\n",
    "# 3️⃣ bucketed_data CTE (create 30-min buckets)\n",
    "bucketed_data = converted_data \\\n",
    "    .filter((col(\"TIMETABLE_VARIATION\") > 0) | (col(\"LATE_IND\") == 1)) \\\n",
    "    .withColumn(\n",
    "        \"time_bucket\",\n",
    "    (floor(col(\"actual_ts\") / 3600) * 3600) + \n",
    "    (floor((col(\"actual_ts\") % 3600) / (30 * 60)) * (30 * 60))\n",
    "    ) \\\n",
    "    .groupBy(\"LOC_STANOX\", \"time_bucket\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"delay_count\")\n",
    "\n",
    "# 4️⃣ top_stations CTE\n",
    "top_stations = bucketed_data \\\n",
    "    .groupBy(\"LOC_STANOX\") \\\n",
    "    .sum(\"delay_count\") \\\n",
    "    .withColumnRenamed(\"sum(delay_count)\", \"total_delay_count\") \\\n",
    "    .orderBy(col(\"total_delay_count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "# 5️⃣ Final SELECT + JOINs\n",
    "# First, join bucketed_data with locations\n",
    "joined_with_locations = bucketed_data.alias(\"b\") \\\n",
    "    .join(locations_raw.alias(\"l\"), col(\"b.LOC_STANOX\") == col(\"l.STANOX\"), how=\"left\") \\\n",
    "    .select(\n",
    "        col(\"b.time_bucket\"),\n",
    "        col(\"b.LOC_STANOX\"),\n",
    "        col(\"b.delay_count\"),\n",
    "        col(\"l.DESCRIPTION\")\n",
    "    )\n",
    "\n",
    "# Now, join with top_stations\n",
    "final_df = joined_with_locations.alias(\"b\") \\\n",
    "    .join(top_stations.alias(\"t\"), col(\"b.LOC_STANOX\") == col(\"t.LOC_STANOX\")) \\\n",
    "    .orderBy(\"b.time_bucket\", \"b.LOC_STANOX\")\n",
    "\n",
    "# Show result\n",
    "top_stations = final_df.select(\n",
    "    col(\"b.time_bucket\").alias(\"time_bucket\"),\n",
    "    col(\"b.LOC_STANOX\").alias(\"LOC_STANOX\"),\n",
    "    col(\"b.delay_count\").alias(\"DELAY_COUNT\"),\n",
    "    col(\"b.DESCRIPTION\").alias(\"DESCRIPTION\"))\n",
    "\n",
    "\n",
    "top_stations.select(to_timestamp(col(\"time_bucket\")/lit(1000)).alias(\"TIME_BUCKET\"),\n",
    "\"LOC_STANOX\",\"DELAY_COUNT\",\"DESCRIPTION\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "df = top_stations.toPandas()\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "# Rename columns to lowercase\n",
    "df = df.rename(columns={\n",
    "    'TIME_BUCKET': 'time_bucket',\n",
    "    'LOC_STANOX': 'loc_stanox',\n",
    "    'DELAY_COUNT': 'delay_count',\n",
    "    'DESCRIPTION': 'description'\n",
    "})\n",
    "\n",
    "# Convert time_bucket to datetime\n",
    "df['time_bucket'] = pd.to_datetime(df['time_bucket'].astype(float) / 1000, unit='s')\n",
    "\n",
    "\n",
    "# Optional: show table to debug\n",
    "print(\"DataFrame head:\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "# Create stacked bar chart\n",
    "chart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('time_bucket:T', title='Time (30-min buckets)'),\n",
    "    y=alt.Y('delay_count:Q', title='Delay Count'),\n",
    "    color=alt.Color('loc_stanox:N', title='Station'),\n",
    "    tooltip=['time_bucket:T', 'loc_stanox:N', 'delay_count:Q']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    title=\"Top Stations by Delay (per 30-min buckets)\"\n",
    ")\n",
    "\n",
    "# Show chart inline\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter last n hours hours\n",
    "HOURS = 4\n",
    "# Get max timestamp in data\n",
    "max_time = df['time_bucket'].max()\n",
    "\n",
    "# Define cutoff time (4 hours ago)\n",
    "cutoff_time = max_time - pd.Timedelta(hours=HOURS)\n",
    "\n",
    "# Filter data\n",
    "df_recent = df[df['time_bucket'] >= cutoff_time]\n",
    "\n",
    "# Optional: print to verify\n",
    "print(f\"Max time in data: {max_time}\")\n",
    "print(f\"Cutoff time: {cutoff_time}\")\n",
    "print(f\"Rows after filter: {len(df_recent)}\")\n",
    "\n",
    "chart = alt.Chart(df_recent).mark_bar().encode(\n",
    "    x=alt.X('time_bucket:T', title='Time (30-min buckets)'),\n",
    "    y=alt.Y('delay_count:Q', title='Delay Count'),\n",
    "    color=alt.Color('loc_stanox:N', title='Station'),\n",
    "    tooltip=['time_bucket:T', 'loc_stanox:N', 'delay_count:Q']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    title=\"Top Stations by Delay (last 4 hours)\"\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b757b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as spark_max, expr, to_timestamp\n",
    "\n",
    "# Delay Data Query\n",
    "\n",
    "# Load DataFrames (replace with your actual loading logic)\n",
    "train_movements = spark.table(f\"HOL_USER_{user_num}_DB.GOLD.TRAIN_MOVEMENTS\").alias(\"tm\")\n",
    "locations_raw = spark.table(f\"HOL_USER_{user_num}_DB.BRONZE.LOCATIONS_RAW\").alias(\"loc\")\n",
    "\n",
    "# Step 1️⃣ - Calculate max_ts\n",
    "max_ts_row = train_movements \\\n",
    "    .filter(col(\"ACTUAL_TIMESTAMP\").isNotNull()) \\\n",
    "    .agg(spark_max(col(\"ACTUAL_TIMESTAMP\")).alias(\"max_ts_epoch\")) \\\n",
    "    .collect()[0]\n",
    "\n",
    "max_ts_epoch = max_ts_row[\"max_ts_epoch\"]\n",
    "\n",
    "\n",
    "# Step 2️⃣ - Build delay_data DataFrame\n",
    "# Assuming m.MVT_LAT_LON is a STRUCT with fields 'lat' and 'long'\n",
    "\n",
    "delay_data = train_movements \\\n",
    "    .filter((col(\"LATE_IND\") == 1) &\n",
    "            (col(\"ACTUAL_TIMESTAMP\").isNotNull()) &\n",
    "            (col(\"MVT_LAT_LON\").isNotNull()) &\n",
    "            (col(\"ACTUAL_TIMESTAMP\") >= lit(max_ts_epoch / 1000))) \\\n",
    "    .join(locations_raw, col(\"tm.LOC_STANOX\") == col(\"loc.STANOX\"), how=\"left\") \\\n",
    "    .select(\n",
    "        col(\"tm.LOC_STANOX\").alias(\"LOC_STANOX\"),\n",
    "        col(\"tm.LATE_IND\").alias(\"LATE_IND\"),\n",
    "        col(\"tm.MVT_LAT_LON.lat\").cast(\"float\").alias(\"LATITUDE\"),\n",
    "        col(\"tm.MVT_LAT_LON.long\").cast(\"float\").alias(\"LONGITUDE\"),\n",
    "        col(\"loc.DESCRIPTION\").alias(\"STATION_NAME\"),\n",
    "        col(\"loc.tiploc\").alias(\"TIPLOC\")\n",
    "    )\n",
    "\n",
    "# Optional: show result\n",
    "delay_data.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d884b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "\n",
    "# Your data processing\n",
    "df = delay_data.toPandas()\n",
    "df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "df[\"LATITUDE\"] = df[\"LATITUDE\"].astype(float)\n",
    "df[\"LONGITUDE\"] = df[\"LONGITUDE\"].astype(float)\n",
    "\n",
    "# Create scatter map (updated function)\n",
    "fig = px.scatter_map(\n",
    "    df,\n",
    "    lat=\"LATITUDE\",\n",
    "    lon=\"LONGITUDE\",\n",
    "    hover_name=\"LATITUDE\",  # You can change this to a more meaningful column if available\n",
    "    hover_data={\"LONGITUDE\": True},\n",
    "    color_discrete_sequence=[\"red\"],\n",
    "    zoom=6,\n",
    "    height=600,\n",
    "    title=\"Train delays in the UK (last 24 hours)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    map=dict(\n",
    "        center=dict(lat=54.0, lon=-2.0),\n",
    "        zoom=6\n",
    "    ),\n",
    "    margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are not running this in your laptop you can try this code\n",
    "# dont try it on a restricted environment like Codespaces\n",
    "SKIP_CELL = True\n",
    "if not SKIP_CELL:\n",
    "\n",
    "    import pandas as pd\n",
    "    import folium\n",
    "    from folium.plugins import MarkerCluster\n",
    "\n",
    "    # Example: Load data from Snowflake\n",
    "    # df = your_snowflake_cursor.to_pandas()\n",
    "    df = delay_data.toPandas()\n",
    "\n",
    "    # Drop nulls in lat/lon\n",
    "    df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "    # Ensure the columns are properly named and lat/lon are numeric\n",
    "    df[\"LATITUDE\"] = df[\"LATITUDE\"].astype(float)\n",
    "    df[\"LONGITUDE\"] = df[\"LONGITUDE\"].astype(float)\n",
    "\n",
    "    # Create a map centered roughly over the UK\n",
    "    m = folium.Map(location=[54.0, -2.0], zoom_start=6)\n",
    "\n",
    "    # Add markers for each train delay\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row[\"LATITUDE\"], row[\"LONGITUDE\"]],\n",
    "            popup=\"Train Delay\"\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "    # Display the map in Jupyter\n",
    "    m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
